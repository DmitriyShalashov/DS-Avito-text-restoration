{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOgB9lAQGzF55xDsvPXorws",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DmitriyShalashov/DS-Avito-text-restoration/blob/main/Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Загрузка библиотек и словарей"
      ],
      "metadata": {
        "id": "Wl2U9QcUKxiP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Предварительно поместите файл для теста в директорию с ноутбуком. Я использовал Google Colab и загружал файл в \"Файлы\" через кнопку \"Загрузить в сессионное хранилище\"."
      ],
      "metadata": {
        "id": "u7BA2gqEKXz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy3"
      ],
      "metadata": {
        "id": "lEw0XcY_zg-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "В качестве словаря для русских слов я использовал корпус OpenCorpora, для английского языка брал словарь из библиотеки nltk. Для того чтобы обрабатывать морфемы слов и приводить их к начальной форме я использовал pymorphy3"
      ],
      "metadata": {
        "id": "xeDFvYUNLJd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import re\n",
        "from itertools import product\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from collections import defaultdict\n",
        "def load_russian_words():\n",
        "          url = \"https://raw.githubusercontent.com/danakt/russian-words/master/russian.txt\"\n",
        "          response = requests.get(url)\n",
        "          if response.status_code == 200:\n",
        "              words = set()\n",
        "              for word in response.text.split('\\n'):\n",
        "                  word = word.strip().lower()\n",
        "                  if word and re.match('^[а-яё]+$', word) and len(word) > 1:\n",
        "                      words.add(word)\n",
        "              print(f\"Загружено {len(words)} русских слов\")\n",
        "              return words\n",
        "corpora = load_russian_words()"
      ],
      "metadata": {
        "id": "lEYT5sjYvSX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('words')\n",
        "from nltk.corpus import words as nltk_words\n",
        "import pymorphy3\n",
        "\n",
        "morph=pymorphy3.MorphAnalyzer()\n",
        "en_dict=set(nltk_words.words())"
      ],
      "metadata": {
        "id": "B_7N4p3UKHd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Создание классов для сегментатора строки и валидатора слов"
      ],
      "metadata": {
        "id": "P5kJypJzK-KY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверка последовательности символов на английское слово, русское слово, словоформу русского слова"
      ],
      "metadata": {
        "id": "vfHaE0RvMp5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WordValidator:\n",
        "    def __init__(self, ru_dict, en_dict, morph):\n",
        "        self.russian_words=ru_dict\n",
        "        self.english_words=en_dict\n",
        "        self.morph_analyzer = morph\n",
        "        self.russian_normal_forms={self.morph_analyzer.parse(word)[0].normal_form for word in ru_dict}\n",
        "\n",
        "    def is_valid_word(self, word):\n",
        "\n",
        "        word_lower = word.lower()\n",
        "\n",
        "        if word_lower in self.english_words:\n",
        "            return True\n",
        "\n",
        "        if word_lower in self.russian_words:\n",
        "            return True\n",
        "\n",
        "        if self.morph_analyzer and self.russian_normal_forms:\n",
        "            try:\n",
        "                parsed = self.morph_analyzer.parse(word_lower)\n",
        "                if parsed:\n",
        "                    normal_form = parsed[0].normal_form\n",
        "                    return normal_form in self.russian_normal_forms\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        return False\n",
        "\n",
        "\n",
        "validator=WordValidator(corpora, en_dict, morph)"
      ],
      "metadata": {
        "id": "YvlOauH5IM6f"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Алгоритм: динамически разбивать строку и оценивать её разбиения с помощью perplexity (метрики естественности языка) и также построить разбиение с помощью жадного алгоритма. Для оценки перплексии я использовал такие модели как rugpt2 и SmolLM3.\n"
      ],
      "metadata": {
        "id": "7SZ5dgfRLsBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from collections import defaultdict\n",
        "\n",
        "class TextRestorer:\n",
        "    def __init__(self, corpora, validator, model_name='sberbank-ai/rugpt3small_based_on_gpt2'):\n",
        "      #HuggingFaceTB/SmolLM3-3B\n",
        "      #sberbank-ai/rugpt3small_based_on_gpt2\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.corpora = set(corpora)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16 if self.device.type == \"cuda\" else torch.float32\n",
        "        )\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "        self.validator = validator\n",
        "\n",
        "\n",
        "    def is_valid_word(self, word: str) -> bool:\n",
        "        return self.validator.is_valid_word(word)\n",
        "\n",
        "    def dp_segmentation(self, text):\n",
        "        n = len(text)\n",
        "        if n == 0:\n",
        "            return []\n",
        "\n",
        "        dp = [None] * (n + 1)\n",
        "        dp[0] = []\n",
        "\n",
        "        for i in range(1, n + 1):\n",
        "            for j in range(max(0, i - 20), i):\n",
        "                word = text[j:i]\n",
        "                if dp[j] is not None and word in self.corpora:\n",
        "                    current_segmentation = dp[j] + [word]\n",
        "\n",
        "                    if dp[i] is None or self.is_better_segmentation(current_segmentation, dp[i]):\n",
        "                        dp[i] = current_segmentation\n",
        "\n",
        "        if dp[n] is not None:\n",
        "            return dp[n]\n",
        "        else:\n",
        "            return self.greedy_segmentation(text)\n",
        "\n",
        "    def is_better_segmentation(self, new_seg, old_seg):\n",
        "        return len(new_seg) < len(old_seg) #Тут можно выбрать другую метрику, например оценить вероятности\n",
        "\n",
        "    def attach_punctuation(self, segments):\n",
        "        if not segments:\n",
        "            return []\n",
        "\n",
        "        result = []\n",
        "        i = 0\n",
        "\n",
        "        while i < len(segments):\n",
        "            current_word = segments[i]\n",
        "            if i < len(segments) - 1 and re.match(r'^[\\W_]+$', segments[i+1]):\n",
        "                punctuation_group = []\n",
        "                j = i + 1\n",
        "                while j < len(segments) and re.match(r'^[\\W_]+$', segments[j]):\n",
        "                    punctuation_group.append(segments[j])\n",
        "                    j += 1\n",
        "\n",
        "                combined = current_word + ''.join(punctuation_group)\n",
        "                result.append(combined)\n",
        "                i = j\n",
        "            else:\n",
        "                result.append(current_word)\n",
        "                i += 1\n",
        "\n",
        "        return result\n",
        "\n",
        "    def get_phrase_probability(self, text):\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\", max_length=128, truncation=True)\n",
        "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs, labels=inputs['input_ids'])\n",
        "            return torch.exp(-outputs.loss).item()\n",
        "\n",
        "    def greedy_segmentation(self, text):\n",
        "        n = len(text)\n",
        "        if n == 0:\n",
        "            return []\n",
        "\n",
        "        for length in range(min(20, n), 0, -1):\n",
        "            word = text[:length]\n",
        "            if word in self.corpora:\n",
        "                remaining_segmentation = self.greedy_segmentation(text[length:])\n",
        "                if remaining_segmentation is not None:\n",
        "                    return [word] + remaining_segmentation\n",
        "\n",
        "        return [text[0]] + self.greedy_segmentation(text[1:]) if n > 1 else [text]\n",
        "\n",
        "    def restore_spaces(self, text):\n",
        "        segments1 = self.dp_segmentation(text)\n",
        "        segments2 = self.greedy_segmentation(text)\n",
        "\n",
        "        segments1_with_punct = self.attach_punctuation(segments1)\n",
        "        segments2_with_punct = self.attach_punctuation(segments2)\n",
        "\n",
        "        restored1_text = ' '.join(segments1_with_punct)\n",
        "        restored2_text = ' '.join(segments2_with_punct)\n",
        "\n",
        "        probability1 = self.get_phrase_probability(restored1_text)\n",
        "        probability2 = self.get_phrase_probability(restored2_text)\n",
        "\n",
        "        probability = probability1 if probability1>probability2 else probability2\n",
        "        restored_text=restored1_text if probability1>probability2 else restored2_text\n",
        "        return {\n",
        "            'restored_text': restored_text,\n",
        "            'probability': probability\n",
        "        }\n",
        "\n"
      ],
      "metadata": {
        "id": "dJGuscs5SHsF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "restorer = TextRestorer(corpora,validator)"
      ],
      "metadata": {
        "id": "L7uuRfGhdRBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Тестирование"
      ],
      "metadata": {
        "id": "rnvFtb4xNDSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_cases = [\n",
        "      \"новыйфильтрдляводы\",\n",
        "      \"сдаюквартирусмебельюитехникой\",\n",
        "      \"ищудомработницу,центр\",\n",
        "      \"Лишьоднаона\",\n",
        "      \"Имнеживётся\"\n",
        "      ]\n",
        "for test_text in test_cases:\n",
        "      result = restorer.restore_spaces(test_text)\n",
        "\n",
        "      print(f\"Исходный текст: {test_text}\")\n",
        "      print(f\"Восстановленный: {result['restored_text']}\")\n",
        "      print(f\"Вероятность: {result['probability']:.6f}\")"
      ],
      "metadata": {
        "id": "DxY0uSMrS-ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "restorer.restore_spaces(\"Однаонаповсюду,гдебынескрылсяя\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8RZgxv-AHfH",
        "outputId": "615637e8-b06d-4991-9db9-6509e40f6f11"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'restored_text': 'О дна она повсюду, где бы неск рылся я',\n",
              " 'probability': 0.0002628164365887642}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def read_special_csv(filename):\n",
        "    ids = []\n",
        "    texts = []\n",
        "\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            comma_pos = line.find(',')\n",
        "            if comma_pos != -1:\n",
        "                id_val = line[:comma_pos]\n",
        "                text_val = line[comma_pos+1:]\n",
        "                ids.append(id_val)\n",
        "                texts.append(text_val)\n",
        "            else:\n",
        "                ids.append(None)\n",
        "                texts.append(line)\n",
        "\n",
        "    return pd.DataFrame({'id': ids, 'text': texts})\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_Xr_EqZETZFT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = read_special_csv('dataset_1937770_3.txt')[1:]"
      ],
      "metadata": {
        "id": "rt_2GU86VgFZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_space_indices(text):\n",
        "    return [i for i, char in enumerate(text) if char == ' ']\n",
        "\n",
        "df[\"text\"]=df[\"text\"].apply(lambda x:find_space_indices(restorer.restore_spaces(x)['restored_text'].lower()))"
      ],
      "metadata": {
        "id": "KB_ngIRdSfOV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.rename(columns={\"text\": \"predicted_positions\"})\n",
        "df[\"predicted_positions\"]=df[\"predicted_positions\"].apply(lambda x:str(x))\n",
        "df.sample(10)"
      ],
      "metadata": {
        "id": "RO3GQlWCWW5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"submission.csv\",  index=False)"
      ],
      "metadata": {
        "id": "uo1rKRxvYXpc"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}